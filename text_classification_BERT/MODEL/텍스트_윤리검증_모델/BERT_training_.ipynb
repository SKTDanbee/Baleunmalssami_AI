{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danbee/anaconda3/envs/danbee/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/danbee/Desktop/Ansim_Keypad/MODEL/í…ìŠ¤íŠ¸_ìœ¤ë¦¬ê²€ì¦_ëª¨ë¸\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danbee/anaconda3/envs/danbee/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = 'BM-K/KoSimCSE-bert-multitask'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"../../DATA/í…ìŠ¤íŠ¸_ìœ¤ë¦¬ê²€ì¦_ë°ì´í„°/emotion_labeld_text.csv\")\n",
    "# df = df[:10000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['is_immoral', 'ë¶ˆí‰/ë¶ˆë§Œ', 'í™˜ì˜/í˜¸ì˜', 'ê°ë™/ê°íƒ„', 'ì§€ê¸‹ì§€ê¸‹', 'ê³ ë§ˆì›€', 'ìŠ¬í””', 'í™”ë‚¨/ë¶„ë…¸',\n",
      "       'ì¡´ê²½', 'ê¸°ëŒ€ê°', 'ìš°ì­ëŒ/ë¬´ì‹œí•¨', 'ì•ˆíƒ€ê¹Œì›€/ì‹¤ë§', 'ë¹„ìž¥í•¨', 'ì˜ì‹¬/ë¶ˆì‹ ', 'ë¿Œë“¯í•¨', 'íŽ¸ì•ˆ/ì¾Œì ',\n",
      "       'ì‹ ê¸°í•¨/ê´€ì‹¬', 'ì•„ê»´ì£¼ëŠ”', 'ë¶€ë„ëŸ¬ì›€', 'ê³µí¬/ë¬´ì„œì›€', 'ì ˆë§', 'í•œì‹¬í•¨', 'ì—­ê²¨ì›€/ì§•ê·¸ëŸ¬ì›€', 'ì§œì¦',\n",
      "       'ì–´ì´ì—†ìŒ', 'ì—†ìŒ', 'íŒ¨ë°°/ìžê¸°í˜ì˜¤', 'ê·€ì°®ìŒ', 'íž˜ë“¦/ì§€ì¹¨', 'ì¦ê±°ì›€/ì‹ ë‚¨', 'ê¹¨ë‹¬ìŒ', 'ì£„ì±…ê°',\n",
      "       'ì¦ì˜¤/í˜ì˜¤', 'íë­‡í•¨(ê·€ì—¬ì›€/ì˜ˆì¨)', 'ë‹¹í™©/ë‚œì²˜', 'ê²½ì•…', 'ë¶€ë‹´/ì•ˆ_ë‚´í‚´', 'ì„œëŸ¬ì›€', 'ìž¬ë¯¸ì—†ìŒ',\n",
      "       'ë¶ˆìŒí•¨/ì—°ë¯¼', 'ë†€ëžŒ', 'í–‰ë³µ', 'ë¶ˆì•ˆ/ê±±ì •', 'ê¸°ì¨', 'ì•ˆì‹¬/ì‹ ë¢°', 'DISCRIMINATION', 'HATE',\n",
      "       'CENSURE', 'VIOLENCE', 'CRIME', 'SEXUAL', 'ABUSE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "labels = df.drop(['conversation_idx', 'sentence_idx', 'origin_text', 'speaker', 'intensity'], axis=1) # intensity ì¶”ê°€ë¨\n",
    "print(labels.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë“  í•­ëª©ì— ëŒ€í•´ 0.5 ì´ìƒì´ë©´ 1, 0.5 ë¯¸ë§Œì´ë©´ 0ìœ¼ë¡œ ë°”ê¾¸ê¸°\n",
    "df = df.apply(lambda x: x if x.name in ['conversation_idx', 'sentence_idx', 'origin_text', 'speaker', 'intensity'] else x.apply(lambda y: 1 if y >= 0.5 else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at BM-K/KoSimCSE-bert-multitask and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/danbee/anaconda3/envs/danbee/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Training Progress: 200it [06:09,  1.85s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                           \n",
      "\u001b[A                                                   \n",
      "\n",
      "Training Progress: 200it [06:14,  1.85s/it]    \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3077302873134613, 'eval_runtime': 4.6227, 'eval_samples_per_second': 78.742, 'eval_steps_per_second': 2.596, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 400it [12:22,  1.84s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                           \n",
      "\u001b[A                                                   \n",
      "\n",
      "Training Progress: 400it [12:27,  1.84s/it]    \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.240280881524086, 'eval_runtime': 4.6583, 'eval_samples_per_second': 78.141, 'eval_steps_per_second': 2.576, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                           \n",
      "Training Progress: 500it [15:31,  1.83s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3447, 'grad_norm': 0.44194236397743225, 'learning_rate': 5e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 600it [18:27,  1.73s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                           \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 600it [18:33,  1.73s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20875456929206848, 'eval_runtime': 6.6671, 'eval_samples_per_second': 54.596, 'eval_steps_per_second': 1.8, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 800it [24:21,  1.74s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                           \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 800it [24:28,  1.74s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19660905003547668, 'eval_runtime': 6.699, 'eval_samples_per_second': 54.336, 'eval_steps_per_second': 1.791, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                            \n",
      "Training Progress: 1000it [30:20,  1.84s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2102, 'grad_norm': 0.5262125134468079, 'learning_rate': 4.925404308647133e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                            \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 1000it [30:24,  1.84s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1964593231678009, 'eval_runtime': 4.7503, 'eval_samples_per_second': 76.627, 'eval_steps_per_second': 2.526, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 1200it [36:30,  1.81s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                            \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 1200it [36:35,  1.81s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1825944185256958, 'eval_runtime': 4.7005, 'eval_samples_per_second': 77.438, 'eval_steps_per_second': 2.553, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 1400it [42:31,  1.84s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                            \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 1400it [42:36,  1.84s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18084824085235596, 'eval_runtime': 5.2303, 'eval_samples_per_second': 69.594, 'eval_steps_per_second': 2.294, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                            \n",
      "Training Progress: 1500it [45:40,  1.82s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1905, 'grad_norm': 0.49089568853378296, 'learning_rate': 4.850808617294265e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 1600it [48:43,  1.83s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                            \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 1600it [48:48,  1.83s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.176579087972641, 'eval_runtime': 4.7541, 'eval_samples_per_second': 76.565, 'eval_steps_per_second': 2.524, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 1800it [54:55,  1.84s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                            \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 1800it [55:00,  1.84s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1696668416261673, 'eval_runtime': 4.6489, 'eval_samples_per_second': 78.298, 'eval_steps_per_second': 2.581, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      "Training Progress: 2000it [1:01:07,  1.83s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1784, 'grad_norm': 0.7243666648864746, 'learning_rate': 4.776212925941398e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 2000it [1:01:12,  1.83s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16931027173995972, 'eval_runtime': 5.1466, 'eval_samples_per_second': 70.726, 'eval_steps_per_second': 2.332, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 2200it [1:07:00,  1.73s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 2200it [1:07:07,  1.73s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1636503040790558, 'eval_runtime': 6.7015, 'eval_samples_per_second': 54.316, 'eval_steps_per_second': 1.791, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 2400it [1:12:56,  1.73s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 2400it [1:13:02,  1.73s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16143931448459625, 'eval_runtime': 6.7113, 'eval_samples_per_second': 54.237, 'eval_steps_per_second': 1.788, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      "Training Progress: 2500it [1:15:56,  1.74s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1735, 'grad_norm': 0.6525942087173462, 'learning_rate': 4.70161723458853e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 2600it [1:18:51,  1.84s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 2600it [1:18:56,  1.84s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16084407269954681, 'eval_runtime': 5.1463, 'eval_samples_per_second': 70.731, 'eval_steps_per_second': 2.332, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 2800it [1:24:58,  1.75s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 2800it [1:25:06,  1.75s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.15777310729026794, 'eval_runtime': 7.5937, 'eval_samples_per_second': 47.935, 'eval_steps_per_second': 1.58, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      "Training Progress: 3000it [1:30:55,  1.75s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1676, 'grad_norm': 0.9249000549316406, 'learning_rate': 4.627021543235663e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 3000it [1:31:02,  1.75s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.15777699649333954, 'eval_runtime': 7.1809, 'eval_samples_per_second': 50.69, 'eval_steps_per_second': 1.671, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 3200it [1:36:52,  1.74s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 3200it [1:36:59,  1.74s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1615857481956482, 'eval_runtime': 7.6332, 'eval_samples_per_second': 47.686, 'eval_steps_per_second': 1.572, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 3400it [1:42:49,  1.74s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 3400it [1:42:55,  1.74s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16076312959194183, 'eval_runtime': 5.9843, 'eval_samples_per_second': 60.826, 'eval_steps_per_second': 2.005, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      "Training Progress: 3500it [1:45:59,  1.83s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1633, 'grad_norm': 0.9036368727684021, 'learning_rate': 4.5524258518827954e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 3600it [1:49:03,  1.84s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 3600it [1:49:08,  1.84s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.157838374376297, 'eval_runtime': 4.7825, 'eval_samples_per_second': 76.111, 'eval_steps_per_second': 2.509, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 3800it [1:54:58,  1.74s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 3800it [1:55:06,  1.74s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.15302199125289917, 'eval_runtime': 7.5908, 'eval_samples_per_second': 47.953, 'eval_steps_per_second': 1.581, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      "Training Progress: 4000it [2:00:55,  1.75s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1606, 'grad_norm': 0.5831063985824585, 'learning_rate': 4.477830160529928e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 4000it [2:01:02,  1.75s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.15334202349185944, 'eval_runtime': 7.5754, 'eval_samples_per_second': 48.05, 'eval_steps_per_second': 1.584, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 4200it [2:06:53,  1.74s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 4200it [2:07:00,  1.74s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14913678169250488, 'eval_runtime': 7.1969, 'eval_samples_per_second': 50.578, 'eval_steps_per_second': 1.667, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 4400it [2:12:49,  1.74s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 4400it [2:12:57,  1.74s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14822062849998474, 'eval_runtime': 7.779, 'eval_samples_per_second': 46.792, 'eval_steps_per_second': 1.543, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      "Training Progress: 4500it [2:15:52,  1.74s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1582, 'grad_norm': 0.8613103628158569, 'learning_rate': 4.4032344691770604e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 4600it [2:18:46,  1.74s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 4600it [2:18:54,  1.74s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14538148045539856, 'eval_runtime': 7.6264, 'eval_samples_per_second': 47.729, 'eval_steps_per_second': 1.573, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 4800it [2:24:42,  1.74s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 4800it [2:24:50,  1.74s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1467045098543167, 'eval_runtime': 7.1939, 'eval_samples_per_second': 50.598, 'eval_steps_per_second': 1.668, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      "Training Progress: 5000it [2:30:39,  1.74s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1539, 'grad_norm': 1.091747522354126, 'learning_rate': 4.328638777824193e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 5000it [2:30:46,  1.74s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14829832315444946, 'eval_runtime': 7.1146, 'eval_samples_per_second': 51.163, 'eval_steps_per_second': 1.687, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 5200it [2:36:36,  1.74s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 5200it [2:36:42,  1.74s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1458890736103058, 'eval_runtime': 6.6958, 'eval_samples_per_second': 54.362, 'eval_steps_per_second': 1.792, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 5400it [2:42:38,  1.84s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 5400it [2:42:43,  1.84s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14576297998428345, 'eval_runtime': 4.8745, 'eval_samples_per_second': 74.674, 'eval_steps_per_second': 2.462, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      "Training Progress: 5500it [2:45:48,  1.84s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1518, 'grad_norm': 0.6616541147232056, 'learning_rate': 4.2540430864713255e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 5600it [2:48:51,  1.83s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 5600it [2:48:56,  1.83s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1434662938117981, 'eval_runtime': 4.8357, 'eval_samples_per_second': 75.274, 'eval_steps_per_second': 2.482, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 5800it [2:55:05,  1.84s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 5800it [2:55:10,  1.84s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14245545864105225, 'eval_runtime': 4.7957, 'eval_samples_per_second': 75.901, 'eval_steps_per_second': 2.502, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      "Training Progress: 6000it [3:01:09,  1.74s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1506, 'grad_norm': 0.4896635115146637, 'learning_rate': 4.179447395118458e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 6000it [3:01:16,  1.74s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1420101374387741, 'eval_runtime': 6.6651, 'eval_samples_per_second': 54.613, 'eval_steps_per_second': 1.8, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 6200it [3:07:06,  1.74s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 6200it [3:07:14,  1.74s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1401752084493637, 'eval_runtime': 7.6575, 'eval_samples_per_second': 47.535, 'eval_steps_per_second': 1.567, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 6400it [3:13:03,  1.75s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 6400it [3:13:10,  1.75s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13945427536964417, 'eval_runtime': 6.8575, 'eval_samples_per_second': 53.081, 'eval_steps_per_second': 1.75, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      "Training Progress: 6500it [3:16:06,  1.76s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1501, 'grad_norm': 0.5782777667045593, 'learning_rate': 4.104851703765591e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 6600it [3:19:01,  1.75s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 6600it [3:19:08,  1.75s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1440873146057129, 'eval_runtime': 7.1824, 'eval_samples_per_second': 50.679, 'eval_steps_per_second': 1.671, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 6800it [3:24:57,  1.75s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 6800it [3:25:05,  1.75s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1391928344964981, 'eval_runtime': 7.7049, 'eval_samples_per_second': 47.242, 'eval_steps_per_second': 1.557, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      "Training Progress: 7000it [3:30:53,  1.73s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1477, 'grad_norm': 0.8131486773490906, 'learning_rate': 4.030256012412723e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 7000it [3:31:01,  1.73s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13908080756664276, 'eval_runtime': 7.5197, 'eval_samples_per_second': 48.406, 'eval_steps_per_second': 1.596, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 7200it [3:36:50,  1.75s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 7200it [3:36:57,  1.75s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1377740353345871, 'eval_runtime': 7.2308, 'eval_samples_per_second': 50.34, 'eval_steps_per_second': 1.66, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 7400it [3:42:45,  1.75s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 7400it [3:42:52,  1.75s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13843576610088348, 'eval_runtime': 6.6542, 'eval_samples_per_second': 54.703, 'eval_steps_per_second': 1.803, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      "Training Progress: 7500it [3:45:47,  1.77s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1466, 'grad_norm': 0.6566901803016663, 'learning_rate': 3.9556603210598555e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 7600it [3:48:50,  1.83s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 7600it [3:48:56,  1.83s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13784180581569672, 'eval_runtime': 5.1469, 'eval_samples_per_second': 70.722, 'eval_steps_per_second': 2.331, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 7800it [3:54:52,  1.75s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 7800it [3:54:59,  1.75s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13911913335323334, 'eval_runtime': 7.1685, 'eval_samples_per_second': 50.778, 'eval_steps_per_second': 1.674, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      "Training Progress: 8000it [4:00:48,  1.75s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1447, 'grad_norm': 0.8343608975410461, 'learning_rate': 3.881064629706989e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 8000it [4:00:56,  1.75s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13747155666351318, 'eval_runtime': 7.104, 'eval_samples_per_second': 51.239, 'eval_steps_per_second': 1.689, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 8200it [4:06:45,  1.74s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "Training Progress: 8200it [4:06:52,  1.74s/it]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13710305094718933, 'eval_runtime': 6.7458, 'eval_samples_per_second': 53.959, 'eval_steps_per_second': 1.779, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 8352it [4:11:18,  1.86s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 109\u001b[0m\n\u001b[1;32m    101\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[1;32m    102\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                         \n\u001b[1;32m    103\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                  \n\u001b[1;32m    104\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,         \n\u001b[1;32m    105\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset,            \n\u001b[1;32m    106\u001b[0m )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 80\u001b[0m, in \u001b[0;36mCustomTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress_bar\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress_bar\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/danbee/lib/python3.10/site-packages/transformers/trainer.py:1948\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1946\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1949\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/danbee/lib/python3.10/site-packages/transformers/trainer.py:2246\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2243\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2245\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2246\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2247\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[0;32m~/anaconda3/envs/danbee/lib/python3.10/site-packages/accelerate/data_loader.py:464\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    463\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[0;32m--> 464\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[0;32m~/anaconda3/envs/danbee/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/danbee/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/danbee/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/danbee/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[8], line 30\u001b[0m, in \u001b[0;36mTextDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     29\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morigin_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconversation_idx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence_idx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43morigin_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspeaker\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mintensity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39miloc[idx]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# intensity ì¶”ê°€ë¨\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39msqueeze() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/anaconda3/envs/danbee/lib/python3.10/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/danbee/lib/python3.10/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/danbee/lib/python3.10/site-packages/pandas/core/generic.py:4869\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4866\u001b[0m     new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   4868\u001b[0m bm_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m axis_num \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 4869\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4872\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4875\u001b[0m \u001b[43m    \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4876\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4877\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_mgr, axes\u001b[38;5;241m=\u001b[39mnew_mgr\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   4878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/danbee/lib/python3.10/site-packages/pandas/core/internals/managers.py:680\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested axis not found in manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[1;32m    689\u001b[0m             indexer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[1;32m    696\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/envs/danbee/lib/python3.10/site-packages/pandas/core/internals/managers.py:843\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[0;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[1;32m    841\u001b[0m                     blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[1;32m    842\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 843\u001b[0m                 nb \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m                 blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n",
      "File \u001b[0;32m~/anaconda3/envs/danbee/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[0;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/danbee/lib/python3.10/site-packages/pandas/core/array_algos/take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[1;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/danbee/lib/python3.10/site-packages/pandas/core/array_algos/take.py:162\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[1;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[1;32m    161\u001b[0m )\n\u001b[0;32m--> 162\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[1;32m    165\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Determine the number of labels\n",
    "num_labels = len(df.columns) - 5  # Subtracting non-label columns: 'conversation_idx', 'sentence_idx', 'origin_text', 'speaker'\n",
    "\n",
    "# Load the model with the correct number of labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "# Train-test split\n",
    "train, val = train_test_split(df, test_size=0.001, random_state=42)\n",
    "\n",
    "# Custom Dataset Class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['origin_text']\n",
    "        labels = self.data.drop(['conversation_idx', 'sentence_idx', 'origin_text', 'speaker', 'intensity'], axis=1).iloc[idx].values.astype('float32') # intensity ì¶”ê°€ë¨\n",
    "        inputs = self.tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze() for k, v in inputs.items()}\n",
    "        inputs['labels'] = torch.tensor(labels)\n",
    "        return inputs\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = TextDataset(train, tokenizer)\n",
    "val_dataset = TextDataset(val, tokenizer)\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.writer = SummaryWriter(log_dir='./logs')\n",
    "        self.progress_bar = tqdm(total=self.args.max_steps, desc=\"Training Progress\")\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = BCEWithLogitsLoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def training_step(self, model, inputs):\n",
    "        loss = super().training_step(model, inputs)\n",
    "        self.writer.add_scalar('Loss/train', loss.item(), self.state.global_step)\n",
    "        self.progress_bar.update(1)\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix: str = \"eval\"):\n",
    "        metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "        self.writer.add_scalar('Loss/validation', metrics['eval_loss'], self.state.global_step)\n",
    "        \n",
    "        # Log other metrics if available\n",
    "        for key, value in metrics.items():\n",
    "            if key.startswith(metric_key_prefix):\n",
    "                self.writer.add_scalar(f'{metric_key_prefix}/{key}', value, self.state.global_step)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def log_metrics(self, split, metrics):\n",
    "        # Log metrics to TensorBoard\n",
    "        for key, value in metrics.items():\n",
    "            self.writer.add_scalar(f'{split}/{key}', value, self.state.global_step)\n",
    "        super().log_metrics(split, metrics)\n",
    "\n",
    "    def train(self, *args, **kwargs):\n",
    "        self.progress_bar.reset()\n",
    "        super().train(*args, **kwargs)\n",
    "        self.progress_bar.close()\n",
    "        self.writer.close()  # Ensure the writer is closed after training\n",
    "\n",
    "# Set up training arguments and the Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=32,  \n",
    "    per_device_eval_batch_size=32,   \n",
    "    warmup_steps=500,                \n",
    "    weight_decay=0.01,               \n",
    "    logging_dir='./logs',            \n",
    "    save_steps=3000,                 \n",
    "    save_total_limit=2,              \n",
    "    report_to=\"tensorboard\",         \n",
    "    eval_steps=200,\n",
    "    # logging_steps=100,  # Ensure logging occurs frequently\n",
    "    evaluation_strategy=\"steps\",  # Evaluate at every eval_steps\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=val_dataset,            \n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train(resume_from_checkpoint=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danbee/anaconda3/envs/danbee/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./results/checkpoint-5000')\n",
    "tokenizer = AutoTokenizer.from_pretrained('BM-K/KoSimCSE-bert-multitask')\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "# Define the test dataset\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['origin_text']\n",
    "        inputs = self.tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze() for k, v in inputs.items()}\n",
    "        return inputs\n",
    "df = pd.read_csv(\"../../DATA/í…ìŠ¤íŠ¸_ìœ¤ë¦¬ê²€ì¦_ë°ì´í„°/emotion_labeld_text.csv\")\n",
    "# Load the test data\n",
    "test_df = df[:100]\n",
    "test_dataset = TestDataset(test_df, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.sigmoid(logits).cpu().numpy()  # Move tensor to CPU before converting to NumPy\n",
    "        predictions.extend(preds)\n",
    "\n",
    "label_columns = ['is_immoral', 'ë¶ˆí‰/ë¶ˆë§Œ', 'í™˜ì˜/í˜¸ì˜', 'ê°ë™/ê°íƒ„', 'ì§€ê¸‹ì§€ê¸‹', 'ê³ ë§ˆì›€',\n",
    "       'ìŠ¬í””', 'í™”ë‚¨/ë¶„ë…¸', 'ì¡´ê²½', 'ê¸°ëŒ€ê°', 'ìš°ì­ëŒ/ë¬´ì‹œí•¨', 'ì•ˆíƒ€ê¹Œì›€/ì‹¤ë§', 'ë¹„ìž¥í•¨', 'ì˜ì‹¬/ë¶ˆì‹ ', 'ë¿Œë“¯í•¨',\n",
    "       'íŽ¸ì•ˆ/ì¾Œì ', 'ì‹ ê¸°í•¨/ê´€ì‹¬', 'ì•„ê»´ì£¼ëŠ”', 'ë¶€ë„ëŸ¬ì›€', 'ê³µí¬/ë¬´ì„œì›€', 'ì ˆë§', 'í•œì‹¬í•¨', 'ì—­ê²¨ì›€/ì§•ê·¸ëŸ¬ì›€',\n",
    "       'ì§œì¦', 'ì–´ì´ì—†ìŒ', 'ì—†ìŒ', 'íŒ¨ë°°/ìžê¸°í˜ì˜¤', 'ê·€ì°®ìŒ', 'íž˜ë“¦/ì§€ì¹¨', 'ì¦ê±°ì›€/ì‹ ë‚¨', 'ê¹¨ë‹¬ìŒ', 'ì£„ì±…ê°',\n",
    "       'ì¦ì˜¤/í˜ì˜¤', 'íë­‡í•¨(ê·€ì—¬ì›€/ì˜ˆì¨)', 'ë‹¹í™©/ë‚œì²˜', 'ê²½ì•…', 'ë¶€ë‹´/ì•ˆ_ë‚´í‚´', 'ì„œëŸ¬ì›€', 'ìž¬ë¯¸ì—†ìŒ',\n",
    "       'ë¶ˆìŒí•¨/ì—°ë¯¼', 'ë†€ëžŒ', 'í–‰ë³µ', 'ë¶ˆì•ˆ/ê±±ì •', 'ê¸°ì¨', 'ì•ˆì‹¬/ì‹ ë¢°', 'DISCRIMINATION', 'HATE',\n",
    "       'CENSURE', 'VIOLENCE', 'CRIME', 'SEXUAL', 'ABUSE']\n",
    "\n",
    "# Convert predictions to a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions, columns=label_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_160967/1672186519.py:17: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictions = pd.concat([predictions, current_prediction], axis=0, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queue</th>\n",
       "      <th>is_immoral</th>\n",
       "      <th>intensity</th>\n",
       "      <th>ë¶ˆí‰/ë¶ˆë§Œ</th>\n",
       "      <th>í™˜ì˜/í˜¸ì˜</th>\n",
       "      <th>ê°ë™/ê°íƒ„</th>\n",
       "      <th>ì§€ê¸‹ì§€ê¸‹</th>\n",
       "      <th>ê³ ë§ˆì›€</th>\n",
       "      <th>ìŠ¬í””</th>\n",
       "      <th>í™”ë‚¨/ë¶„ë…¸</th>\n",
       "      <th>...</th>\n",
       "      <th>í–‰ë³µ</th>\n",
       "      <th>ë¶ˆì•ˆ/ê±±ì •</th>\n",
       "      <th>ê¸°ì¨</th>\n",
       "      <th>ì•ˆì‹¬/ì‹ ë¢°</th>\n",
       "      <th>DISCRIMINATION</th>\n",
       "      <th>HATE</th>\n",
       "      <th>CENSURE</th>\n",
       "      <th>VIOLENCE</th>\n",
       "      <th>CRIME</th>\n",
       "      <th>SEXUAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ë„ˆ</td>\n",
       "      <td>0.179760</td>\n",
       "      <td>0.038211</td>\n",
       "      <td>0.350679</td>\n",
       "      <td>0.096618</td>\n",
       "      <td>0.046731</td>\n",
       "      <td>0.314059</td>\n",
       "      <td>0.015149</td>\n",
       "      <td>0.033535</td>\n",
       "      <td>0.260198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024146</td>\n",
       "      <td>0.264991</td>\n",
       "      <td>0.044902</td>\n",
       "      <td>0.070248</td>\n",
       "      <td>0.030641</td>\n",
       "      <td>0.026364</td>\n",
       "      <td>0.165029</td>\n",
       "      <td>0.004865</td>\n",
       "      <td>0.007795</td>\n",
       "      <td>0.021417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ë„ˆ ì •ë§</td>\n",
       "      <td>0.242867</td>\n",
       "      <td>0.089174</td>\n",
       "      <td>0.498137</td>\n",
       "      <td>0.036678</td>\n",
       "      <td>0.135842</td>\n",
       "      <td>0.157985</td>\n",
       "      <td>0.010959</td>\n",
       "      <td>0.156921</td>\n",
       "      <td>0.577039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015450</td>\n",
       "      <td>0.311582</td>\n",
       "      <td>0.030134</td>\n",
       "      <td>0.024001</td>\n",
       "      <td>0.022595</td>\n",
       "      <td>0.036645</td>\n",
       "      <td>0.202873</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.006627</td>\n",
       "      <td>0.012880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ë„ˆ ì •ë§ ê°œ</td>\n",
       "      <td>0.600446</td>\n",
       "      <td>0.396136</td>\n",
       "      <td>0.397350</td>\n",
       "      <td>0.042395</td>\n",
       "      <td>0.135958</td>\n",
       "      <td>0.089429</td>\n",
       "      <td>0.007664</td>\n",
       "      <td>0.088975</td>\n",
       "      <td>0.597812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013392</td>\n",
       "      <td>0.288172</td>\n",
       "      <td>0.028436</td>\n",
       "      <td>0.016959</td>\n",
       "      <td>0.053053</td>\n",
       "      <td>0.100659</td>\n",
       "      <td>0.388168</td>\n",
       "      <td>0.017173</td>\n",
       "      <td>0.013358</td>\n",
       "      <td>0.029789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ë„ˆ ì •ë§ ê°œìƒˆë¼</td>\n",
       "      <td>0.663998</td>\n",
       "      <td>0.999970</td>\n",
       "      <td>0.367194</td>\n",
       "      <td>0.073576</td>\n",
       "      <td>0.155910</td>\n",
       "      <td>0.065485</td>\n",
       "      <td>0.012745</td>\n",
       "      <td>0.085823</td>\n",
       "      <td>0.587879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025691</td>\n",
       "      <td>0.189923</td>\n",
       "      <td>0.051150</td>\n",
       "      <td>0.013936</td>\n",
       "      <td>0.043211</td>\n",
       "      <td>0.112162</td>\n",
       "      <td>0.489692</td>\n",
       "      <td>0.022525</td>\n",
       "      <td>0.014018</td>\n",
       "      <td>0.031690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ë„ˆ ì •ë§ ê°œìƒˆë¼êµ¬ë‚˜</td>\n",
       "      <td>0.630266</td>\n",
       "      <td>0.824170</td>\n",
       "      <td>0.312282</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.317904</td>\n",
       "      <td>0.063481</td>\n",
       "      <td>0.012726</td>\n",
       "      <td>0.085510</td>\n",
       "      <td>0.517674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028518</td>\n",
       "      <td>0.190189</td>\n",
       "      <td>0.072226</td>\n",
       "      <td>0.021714</td>\n",
       "      <td>0.054911</td>\n",
       "      <td>0.101805</td>\n",
       "      <td>0.482196</td>\n",
       "      <td>0.015326</td>\n",
       "      <td>0.013765</td>\n",
       "      <td>0.043446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        queue  is_immoral  intensity     ë¶ˆí‰/ë¶ˆë§Œ     í™˜ì˜/í˜¸ì˜     ê°ë™/ê°íƒ„      ì§€ê¸‹ì§€ê¸‹  \\\n",
       "0           ë„ˆ    0.179760   0.038211  0.350679  0.096618  0.046731  0.314059   \n",
       "1        ë„ˆ ì •ë§    0.242867   0.089174  0.498137  0.036678  0.135842  0.157985   \n",
       "2      ë„ˆ ì •ë§ ê°œ    0.600446   0.396136  0.397350  0.042395  0.135958  0.089429   \n",
       "3    ë„ˆ ì •ë§ ê°œìƒˆë¼    0.663998   0.999970  0.367194  0.073576  0.155910  0.065485   \n",
       "4  ë„ˆ ì •ë§ ê°œìƒˆë¼êµ¬ë‚˜    0.630266   0.824170  0.312282  0.074153  0.317904  0.063481   \n",
       "\n",
       "        ê³ ë§ˆì›€        ìŠ¬í””     í™”ë‚¨/ë¶„ë…¸  ...        í–‰ë³µ     ë¶ˆì•ˆ/ê±±ì •        ê¸°ì¨     ì•ˆì‹¬/ì‹ ë¢°  \\\n",
       "0  0.015149  0.033535  0.260198  ...  0.024146  0.264991  0.044902  0.070248   \n",
       "1  0.010959  0.156921  0.577039  ...  0.015450  0.311582  0.030134  0.024001   \n",
       "2  0.007664  0.088975  0.597812  ...  0.013392  0.288172  0.028436  0.016959   \n",
       "3  0.012745  0.085823  0.587879  ...  0.025691  0.189923  0.051150  0.013936   \n",
       "4  0.012726  0.085510  0.517674  ...  0.028518  0.190189  0.072226  0.021714   \n",
       "\n",
       "   DISCRIMINATION      HATE   CENSURE  VIOLENCE     CRIME    SEXUAL  \n",
       "0        0.030641  0.026364  0.165029  0.004865  0.007795  0.021417  \n",
       "1        0.022595  0.036645  0.202873  0.005243  0.006627  0.012880  \n",
       "2        0.053053  0.100659  0.388168  0.017173  0.013358  0.029789  \n",
       "3        0.043211  0.112162  0.489692  0.022525  0.014018  0.031690  \n",
       "4        0.054911  0.101805  0.482196  0.015326  0.013765  0.043446  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming the model is already moved to the GPU\n",
    "\n",
    "examples = ['ë„ˆ', 'ë„ˆ ì •ë§', 'ë„ˆ ì •ë§ ê°œ', 'ë„ˆ ì •ë§ ê°œìƒˆë¼', \"ë„ˆ ì •ë§ ê°œìƒˆë¼êµ¬ë‚˜\"]\n",
    "predictions = pd.DataFrame(columns=label_columns)\n",
    "\n",
    "for example in examples:\n",
    "    inputs = tokenizer(example)\n",
    "    inputs = {k: torch.tensor(v).unsqueeze(0).to(device) for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    outputs = model(**inputs)\n",
    "    preds = torch.sigmoid(outputs.logits).detach().cpu().numpy()  # Detach tensor before converting to NumPy\n",
    "    \n",
    "    # Create a DataFrame for the current prediction\n",
    "    current_prediction = pd.DataFrame(preds, columns=label_columns)  # Exclude the \"queue\" column\n",
    "    current_prediction[\"queue\"] = example  # Add the \"queue\" column\n",
    "    \n",
    "    # Concatenate the current prediction to the predictions DataFrame\n",
    "    predictions = pd.concat([predictions, current_prediction], axis=0, ignore_index=True)\n",
    "\n",
    "# Reorder columns to have \"queue\" as the first column\n",
    "predictions = predictions[[\"queue\"] + label_columns[:-1]]\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27425/1235757157.py:17: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictions = pd.concat([predictions, current_prediction], axis=0, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queue</th>\n",
       "      <th>is_immoral</th>\n",
       "      <th>intensity</th>\n",
       "      <th>ë¶ˆí‰/ë¶ˆë§Œ</th>\n",
       "      <th>í™˜ì˜/í˜¸ì˜</th>\n",
       "      <th>ê°ë™/ê°íƒ„</th>\n",
       "      <th>ì§€ê¸‹ì§€ê¸‹</th>\n",
       "      <th>ê³ ë§ˆì›€</th>\n",
       "      <th>ìŠ¬í””</th>\n",
       "      <th>í™”ë‚¨/ë¶„ë…¸</th>\n",
       "      <th>...</th>\n",
       "      <th>ë¶ˆì•ˆ/ê±±ì •</th>\n",
       "      <th>ê¸°ì¨</th>\n",
       "      <th>ì•ˆì‹¬/ì‹ ë¢°</th>\n",
       "      <th>DISCRIMINATION</th>\n",
       "      <th>HATE</th>\n",
       "      <th>CENSURE</th>\n",
       "      <th>VIOLENCE</th>\n",
       "      <th>CRIME</th>\n",
       "      <th>SEXUAL</th>\n",
       "      <th>ABUSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ë„ˆí¬</td>\n",
       "      <td>0.101134</td>\n",
       "      <td>0.196528</td>\n",
       "      <td>0.376477</td>\n",
       "      <td>0.152148</td>\n",
       "      <td>0.107029</td>\n",
       "      <td>0.296697</td>\n",
       "      <td>0.043427</td>\n",
       "      <td>0.073531</td>\n",
       "      <td>0.258001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232043</td>\n",
       "      <td>0.060481</td>\n",
       "      <td>0.137384</td>\n",
       "      <td>0.024003</td>\n",
       "      <td>0.028506</td>\n",
       "      <td>0.123584</td>\n",
       "      <td>0.015065</td>\n",
       "      <td>0.015356</td>\n",
       "      <td>0.026285</td>\n",
       "      <td>0.017245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ë„ˆí¬ ì§‘</td>\n",
       "      <td>0.081646</td>\n",
       "      <td>0.158600</td>\n",
       "      <td>0.266389</td>\n",
       "      <td>0.198563</td>\n",
       "      <td>0.128230</td>\n",
       "      <td>0.199411</td>\n",
       "      <td>0.057940</td>\n",
       "      <td>0.074929</td>\n",
       "      <td>0.155653</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208553</td>\n",
       "      <td>0.114259</td>\n",
       "      <td>0.166078</td>\n",
       "      <td>0.025508</td>\n",
       "      <td>0.025466</td>\n",
       "      <td>0.080489</td>\n",
       "      <td>0.016953</td>\n",
       "      <td>0.015905</td>\n",
       "      <td>0.030830</td>\n",
       "      <td>0.016755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ë„ˆí¬ ì§‘ ê°œ</td>\n",
       "      <td>0.654950</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.308796</td>\n",
       "      <td>0.125560</td>\n",
       "      <td>0.150157</td>\n",
       "      <td>0.210892</td>\n",
       "      <td>0.025698</td>\n",
       "      <td>0.100603</td>\n",
       "      <td>0.260516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175678</td>\n",
       "      <td>0.097965</td>\n",
       "      <td>0.057927</td>\n",
       "      <td>0.063563</td>\n",
       "      <td>0.085728</td>\n",
       "      <td>0.513005</td>\n",
       "      <td>0.027069</td>\n",
       "      <td>0.019872</td>\n",
       "      <td>0.082977</td>\n",
       "      <td>0.028723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ë„ˆí¬ ì§‘ ê°œ ê·€ì—½ë‹¤</td>\n",
       "      <td>0.167840</td>\n",
       "      <td>0.332623</td>\n",
       "      <td>0.090511</td>\n",
       "      <td>0.487590</td>\n",
       "      <td>0.556810</td>\n",
       "      <td>0.067602</td>\n",
       "      <td>0.147902</td>\n",
       "      <td>0.098262</td>\n",
       "      <td>0.069431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113365</td>\n",
       "      <td>0.544671</td>\n",
       "      <td>0.336895</td>\n",
       "      <td>0.047109</td>\n",
       "      <td>0.045260</td>\n",
       "      <td>0.159449</td>\n",
       "      <td>0.042371</td>\n",
       "      <td>0.053288</td>\n",
       "      <td>0.078950</td>\n",
       "      <td>0.055817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        queue  is_immoral  intensity     ë¶ˆí‰/ë¶ˆë§Œ     í™˜ì˜/í˜¸ì˜     ê°ë™/ê°íƒ„      ì§€ê¸‹ì§€ê¸‹  \\\n",
       "0          ë„ˆí¬    0.101134   0.196528  0.376477  0.152148  0.107029  0.296697   \n",
       "1        ë„ˆí¬ ì§‘    0.081646   0.158600  0.266389  0.198563  0.128230  0.199411   \n",
       "2      ë„ˆí¬ ì§‘ ê°œ    0.654950   0.999990  0.308796  0.125560  0.150157  0.210892   \n",
       "3  ë„ˆí¬ ì§‘ ê°œ ê·€ì—½ë‹¤    0.167840   0.332623  0.090511  0.487590  0.556810  0.067602   \n",
       "\n",
       "        ê³ ë§ˆì›€        ìŠ¬í””     í™”ë‚¨/ë¶„ë…¸  ...     ë¶ˆì•ˆ/ê±±ì •        ê¸°ì¨     ì•ˆì‹¬/ì‹ ë¢°  \\\n",
       "0  0.043427  0.073531  0.258001  ...  0.232043  0.060481  0.137384   \n",
       "1  0.057940  0.074929  0.155653  ...  0.208553  0.114259  0.166078   \n",
       "2  0.025698  0.100603  0.260516  ...  0.175678  0.097965  0.057927   \n",
       "3  0.147902  0.098262  0.069431  ...  0.113365  0.544671  0.336895   \n",
       "\n",
       "   DISCRIMINATION      HATE   CENSURE  VIOLENCE     CRIME    SEXUAL     ABUSE  \n",
       "0        0.024003  0.028506  0.123584  0.015065  0.015356  0.026285  0.017245  \n",
       "1        0.025508  0.025466  0.080489  0.016953  0.015905  0.030830  0.016755  \n",
       "2        0.063563  0.085728  0.513005  0.027069  0.019872  0.082977  0.028723  \n",
       "3        0.047109  0.045260  0.159449  0.042371  0.053288  0.078950  0.055817  \n",
       "\n",
       "[4 rows x 54 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming the model is already moved to the GPU\n",
    "\n",
    "examples = [\"ë„ˆí¬\", \"ë„ˆí¬ ì§‘\", \"ë„ˆí¬ ì§‘ ê°œ\", \"ë„ˆí¬ ì§‘ ê°œ ê·€ì—½ë‹¤\" ]\n",
    "predictions = pd.DataFrame(columns=label_columns)\n",
    "\n",
    "for example in examples:\n",
    "    inputs = tokenizer(example)\n",
    "    inputs = {k: torch.tensor(v).unsqueeze(0).to(device) for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    outputs = model(**inputs)\n",
    "    preds = torch.sigmoid(outputs.logits).detach().cpu().numpy()  # Detach tensor before converting to NumPy\n",
    "    \n",
    "    # Create a DataFrame for the current prediction\n",
    "    current_prediction = pd.DataFrame(preds, columns=label_columns[:-1])  # Exclude the \"queue\" column\n",
    "    current_prediction[\"queue\"] = example  # Add the \"queue\" column\n",
    "    \n",
    "    # Concatenate the current prediction to the predictions DataFrame\n",
    "    predictions = pd.concat([predictions, current_prediction], axis=0, ignore_index=True)\n",
    "\n",
    "# Reorder columns to have \"queue\" as the first column\n",
    "predictions = predictions[[\"queue\"] + label_columns[:-1]]\n",
    "\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
